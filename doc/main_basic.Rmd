---
title: "Advanced Data Science Project 4 Report"
fig_crop: no
date: "20/04/2020"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
  word_document: default
sansfont: Arial
header-includes:
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \renewcommand{\contentsname}{}\vspace{-2cm}
- \usepackage{pdfpages}
- \usepackage{bbm}
toc: yes
---
\newpage

# Introduction
In this project, our group implement matrix factorization by focusing on different versions of alternating least squares algorithm for different Regularizations and Kernel Ridge Regression as Post-processing. The objective is to produce a good prediction of usersâ€™ preferences for movies on on the MovieLens dataset. For this project, our team #4 is assigned with Pairing combination 12 + 14 from the Collaborative. For evaluation, we compared RMSE results for different methods. Our group used R language to product model and reports. The specific technique used for the combination are illustrated below. For more details for methods, please also see (https://github.com/TZstatsADS/ADS_Teaching/blob/master/Projects_StarterCodes/Project4-RecommenderSystem/doc/Proj4_pairings_2020_Spring.pdf).


# Models

`Alternating Least Squares + Kernel Ridge Regression`

`Alternating Least Squares + Penalty of magnitudes + Temporal Dynamics + Kernel Ridge Regression`

\newpage

# Model 1: Alternating Least Squares + Kernel Ridge Regression


## Equations recap
### Alternating Least Squares

$$min_{q^{\star}p^{\star}} \sum_{(u,i) \in K} (r_ui-q_i^Tp_u)^2 + \lambda(\sum _i n_{q_i} \| q_i\|^2 +\sum _i n_{p_u} \| p_u\|^2 )$$
ALS technique rotate between fixing the $q_i$'s and fixing the $p_u$'s. When all pu's are fixed, system recomputes the $q_i$'s by solving a least-squares problem, and vice versa. This ensures that each step decreases object function until convergence. 

$f$: dimension of latent factors

$q_i$: factors associated with item i , measures the extent to which items possesses those factors

$p_u$: factors associated with user u, measures the extent of interest that user has in an item  are high on corresponding factors.


### Post-processing

Kernel Ridge Regression:
$$\hat{\beta} = (X^{T}X + \lambda I)^{-1}X^{T}y $$
$$\hat{y_{i}} = {x_i}^T\hat{\beta}$$
Equilvalent to:
$$\hat{\beta} = X^T(XX^T + \lambda I)^{-1}y$$

$$\hat{y}_{i} = K(x_i^{T}, X)(K(X,X) + \lambda I)^{-1}y$$
$y$: vector of movies rated by user i
$X$: a matrix of observations - each row of X is normalized vector of features of one movie j rated by user i: 
$$x_{j_2} = \frac{q_j}{\|q_j\|}$$

$K$: 
$$K(x_{i}^T, X) = x_i^Tx_j$$ 
$$K(x_{i}^T, X) = \exp(2(x_i^Tx_j -1))$$
$\lambda$:
$$\lambda = 0.5$$
We first defined similarity between movies by the above formula. Then we used this similarity S to apply Kernel Ridge Regression prediction. 

### Codes and Details
# \includepdf[pages={-}]{Project_4_ALSA3-.pdf}
```{r, include=FALSE}
library(tinytex)
options(tinytex.verbose = TRUE)

```


# Model 2:Alternating Least Squares + Penalty of magnitudes + Temporal Dynamics + Kernel Ridge Regression
## Equations recap
### Alternating Least Squares

$$min_{q^{\star}p^{\star}} \sum_{(u,i) \in K} (r_ui-q_i^Tp_u)^2 + \lambda(\sum _i n_{q_i} \| q_i\|^2 +\sum _i n_{p_u} \| p_u\|^2 )$$
ALS technique rotate between fixing the $q_i$'s and fixing the $p_u$'s. When all pu's are fixed, system recomputes the $q_i$'s by solving a least-squares problem, and vice versa. This ensures that each step decreases object function until convergence. 

$f$: dimension of latent factors

$q_i$: factors associated with item i , measures the extent to which items possesses those factors

$p_u$: factors associated with user u, measures the extent of interest that user has in an item  are high on corresponding factors.


### Regularizations 

1. Penalty of magnitudes:

$$\sum_{(u,i)\in K} \lambda (\|q_i\|^2+\|p_u\|^2)$$ 
The constant lambda controls the extent of regularization and we determined it through cross validation. 


2.Temporal Dynamics

$$\hat{r}_ui = \mu + b_u + b_i + b_{i.Bin(t)}+q_i^Tp_u$$ 
Since users' preferences and products' popularity are changing over time. We have to consider time factor so add a new variable time to our previous function.For each bin, we used one year of the day and we get 23 bins in total.


### Post-processing

Kernel Ridge Regression:
$$\hat{\beta} = (X^{T}X + \lambda I)^{-1}X^{T}y $$
$$\hat{y_{i}} = {x_i}^T\hat{\beta}$$
Equilvalent to:
$$\hat{\beta} = X^T(XX^T + \lambda I)^{-1}y$$ 

$$\hat{y}_{i} = K(x_i^{T}, X)(K(X,X) + \lambda I)^{-1}y$$ 
$y$: vector of movies rated by user i
$X$: a matrix of observations - each row of X is normalized vector of features of one movie j rated by user i: 
$$x_{j_2} = \frac{q_j}{\|q_j\|}$$ 

$K$: 
$$K(x_{i}^T, X) = x_i^Tx_j$$ 
$$K(x_{i}^T, X) = \exp(2(x_i^Tx_j -1))$$
$\lambda$:
$$\lambda = 0.5$$
### Codes and Details
## \includepdf[pages={-}]{Project_4_ALSR1R2.pdf}

We first defined similarity between movies by the above formula. Then we used this similarity S to apply Kernel Ridge Regression prediction. 

\newpage

# Conclusions
Methods       | f |$\lambda$  |Max Iteration   | Train RMSE | Test RMSE
------------- | -------------|------------- | -------------|-------------|-------------
A3+P3  | 10| 0.1 | 5 | 0.5893|1.3447|-------------
A3+R1R3+P2  | 20 | 10 | 4 | 0.5926| 1.3426



