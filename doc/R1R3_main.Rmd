---
title: "ALS+R1R3+KRR"
author: "Group 4"
output: pdf_document
---

```{r set up, include = F}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(cache = T)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
options(mc.cores = parallel::detectCores())
library(tidyverse)
library(ggplot2)
library(anytime)
library(caret)
library(parallel)
library(krr)

cl <- makeCluster(4)
```

#Model 2:Alternating Least Squares + Penalty of magnitudes + Temporal Dynamics + Kernel Ridge Regression

## Step 1. Load Data and Train-Test Split
In this step, we loaded the data and added variable timediff for temporal dynamics, which is the difference between the user's rating time and the average rating time of this user. Then we splitted the original dataset to training set(80%) and test set(20%)
```{r}
data <- read.csv("../data/ml-latest-small/ratings.csv")
# Convert timestamp and set up time difference
data$timestamp <- anydate(data$timestamp)
data <- data %>% 
  group_by(userId) %>% 
  mutate(timediff = (timestamp - mean(timestamp)) %>% as.numeric) %>% 
  ungroup() %>%
  arrange(timestamp)

set.seed(1)

# train-test split (.8/.2)
train_ind <- createDataPartition(data$userId, p=.8, list=F)


data_train <- data[train_ind, ]
data_test <- data[-train_ind, ]
```

## Step 2. Model Setup
### 1. Equations recap
### Alternating Least Squares

$$min_{q^{\star}p^{\star}} \sum_{(u,i) \in K} (r_ui-q_i^Tp_u)^2 + \lambda(\sum _i n_{q_i} \| q_i\|^2 +\sum _i n_{p_u} \| p_u\|^2 )$$
ALS technique rotate between fixing the $q_i$'s and fixing the $p_u$'s. When all pu's are fixed, system recomputes the $q_i$'s by solving a least-squares problem, and vice versa. This ensures that each step decreases object function until convergence. 

$f$: dimension of latent factors

$q_i$: factors associated with item i , measures the extent to which items possesses those factors

$p_u$: factors associated with user u, measures the extent of interest that user has in an item  are high on corresponding factors.


### Regularizations 
 - Penalty of magnitudes:

$$\sum_{(u,i)\in K} \lambda (\|q_i\|^2+\|p_u\|^2)$$ 
The constant lambda controls the extent of regularization and we determined it through cross validation. 

 - Temporal Dynamics

$$\hat{r}_{ui} = q_i^Tp_u + \mu + b_u + b_i + \alpha_udev_u(t)$$ 
The $dev_u(t)$ is the related to the difference between rating time of user and their average rating time, measured by 
$$dev_u(t) = sign(t - t_u)Â·|t-t_u|^\beta$$ 

The final objective function of our model 2 is
$$min_{q,p} \sum_{(u,i) \in K} [r_{ui}-(q_i^Tp_u + \mu + b_u + b_i + \alpha_udev_u(t)]^2 + \lambda\sum( \| q_i\|^2 +\| p_u\|^2 + b_i^2 + b_u^2 + \alpha_u^2)$$

### Post-processing
Kernel Ridge Regression:
$$\hat{\beta} = (X^{T}X + \lambda I)^{-1}X^{T}y $$
$$\hat{y_{i}} = {x_i}^T\hat{\beta}$$
Equilvalent to:
$$\hat{\beta} = X^T(XX^T + \lambda I)^{-1}y$$ 

$$\hat{y}_{i} = K(x_i^{T}, X)(K(X,X) + \lambda I)^{-1}y$$ 
$y$: vector of movies rated by user i
$X$: a matrix of observations - each row of X is normalized vector of features of one movie j rated by user i: 
$$x_{j_2} = \frac{q_j}{\|q_j\|}$$ 

$K$: 
$$K(x_{i}^T, X) = x_i^Tx_j$$ 
$$K(x_{i}^T, X) = \exp(2(x_i^Tx_j -1))$$
$\lambda$:
$$\lambda = 0.5$$

### 2. Algorithm Setup
For matrix factorization, with the method in paper4, to include $b_u$, $b_i$, and $a_u$ in the iterations, we add one row to the movie matrix and two rows to the user matrix. To speed up the algorithm, we use parallel package to parallelize computations and tibble in the factorization function.

### 3. Parameter Tuning
To save the time, we use the refult of parameters tuning of ALS and KRR from the former model.
```{r}


```


### 4. Optimal Model
 - Model Fitting
```{r}

```
 - Model Evaluation

